{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35785368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmanninen/miniconda3/envs/akida_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-23 11:02:33.127569: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-23 11:02:33.181434: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-23 11:02:33.181502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-23 11:02:33.184346: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-23 11:02:33.199391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-23 11:02:34.224635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/kmanninen/miniconda3/envs/akida_env/lib/python3.11/site-packages/onnxscript/converter.py:816: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "\n",
    "from cnn2snn import set_akida_version, AkidaVersion\n",
    "from akida import Model\n",
    "import akida\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "deviceCheckbox = [\"CPU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31006aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gauge(value):\n",
    "    fig = go.Figure(go.Indicator(\n",
    "        mode=\"gauge+number\",\n",
    "        value=value,\n",
    "        gauge={'axis': {'range': [0, 30]}},\n",
    "        domain={'x': [0, 1], 'y': [0, 1]},\n",
    "    ))\n",
    "    fig.update_layout(width=400, height=300)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0507dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax for an array of values\n",
    "def softmaxArray(values):\n",
    "    # Assuming array shape is (1, 1, 1, x), flatten to get the values\n",
    "    values = values.ravel()\n",
    "    exp_values = np.exp(values)\n",
    "    sum_exp = np.sum(exp_values)\n",
    "    softmax_values = exp_values / sum_exp\n",
    "    return softmax_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48246e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_x = 96\n",
    "image_y = 96\n",
    "image_z = 3\n",
    "labels = [\"no person\", \"person\"]\n",
    "def decodeOutput(inp):\n",
    "        global akida_model\n",
    "        inp = cv2.resize(inp, (image_x, image_y))\n",
    "        inp = inp.reshape((-1, image_x, image_y, image_z))\n",
    "        timer_start = time.time()\n",
    "        predictions = softmaxArray(akida_model.predict(inp))\n",
    "        frame_time = time.time() - timer_start\n",
    "        fps = 1 / frame_time if frame_time > 0 else 0\n",
    "        confidences = {labels[i]: predictions[i] for i in range(len(predictions))}\n",
    "\n",
    "        return confidences, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bccd78e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(inp):\n",
    "\n",
    "  confidences, fps = decodeOutput(inp)\n",
    "\n",
    "  return confidences, create_gauge(round(fps, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ea800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Akida devices found, running on CPU.\n"
     ]
    }
   ],
   "source": [
    "with set_akida_version(AkidaVersion.v2):\n",
    "            global akida_model\n",
    "            akida_model = Model(\"/home/kmanninen/akida2_fpgacloud_examples/models/akidanet_vww_i8_w8_a8.fbz\")\n",
    "            devices = akida.devices()\n",
    "            if len(devices) > 0:\n",
    "                print(f'Available devices: {[dev.desc for dev in devices]}')\n",
    "                device = devices[0]\n",
    "                print(device.version)\n",
    "                try:\n",
    "                    akida_model.map(device)\n",
    "                    print(f\"Mapping to Akida device {device.desc}.\")\n",
    "                    mappedDevice = \"FPGA\"\n",
    "                except Exception as e:\n",
    "                    print(\"Model not compatible with FPGA. Running on CPU.\")\n",
    "                    mappedDevice = \"CPU\"\n",
    "            else:\n",
    "                print(\"No Akida devices found, running on CPU.\")\n",
    "                mappedDevice = \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "237e82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceCheckbox = [mappedDevice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b0e8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "theme = gr.themes.Base(\n",
    "    text_size=\"sm\",\n",
    "    spacing_size=\"sm\",\n",
    "    radius_size=\"sm\",\n",
    ")\n",
    "\n",
    "with gr.Blocks(\n",
    "    title=\"Brainchip\",\n",
    "    fill_width=True,\n",
    "    fill_height=True,\n",
    "    delete_cache=[180, 600],\n",
    "    theme=theme\n",
    ") as demo:\n",
    "    with gr.Tabs():\n",
    "        with gr.Tab(\"Image Classification\") as image_classification_tab:\n",
    "            gr.Markdown(\"\"\"\n",
    "                <h1 style=\"text-align: center;\">Akida Cloud</h1>\n",
    "                <br>\n",
    "                \"\"\")\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\"## Image Classification\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    input_img = gr.Image(sources=[\"webcam\"], type=\"numpy\")\n",
    "                with gr.Column():\n",
    "                    checkbox_group = gr.CheckboxGroup(choices=[\"FPGA\", \"CPU\"], value=deviceCheckbox, label=\"Device\", interactive=False)\n",
    "                    output_label = gr.Label(num_top_classes=3)\n",
    "                    plot = gr.Plot(label=\"Frames per second\")\n",
    "                dep = input_img.stream(classify_image, [input_img], [output_label, plot],\n",
    "                                        time_limit=30, stream_every=0.1, concurrency_limit=30)        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(allowed_paths=[\"img/\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "akida_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
