{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Gesture recognition with spatiotemporal models\n",
        "\n",
        "A tutorial on designing efficient models for streaming video tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction: why spatiotemporal models?\n",
        "\n",
        "Recognizing gestures from video is a challenging task that requires understanding not just\n",
        "individual frames but how those frames evolve over time. Traditional 2D convolutional neural\n",
        "networks (CNNs) are limited here — they analyze only spatial features and discard temporal\n",
        "continuity. 3D CNNs, while well suited to the task, are on the other hand computationally heavy.\n",
        "\n",
        "To tackle this, we turn to lightweight spatiotemporal models, specifically designed to process\n",
        "patterns in both space (image structure) and time (motion, rhythm). These models are essential\n",
        "for tasks like:\n",
        "\n",
        "* Gesture classification\n",
        "* Online eye-tracking\n",
        "* Real-time activity detection in video streams\n",
        "\n",
        "At the heart of these models lies a simple idea: decoupling spatial and temporal analysis,\n",
        "enables efficient, real-time detection — even on resource-constrained devices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Spatiotemporal blocks: the core concept\n",
        "\n",
        "Rather than using full, computationally expensive 3D convolutions, our spatiotemporal blocks break\n",
        "the operation into two parts, a:\n",
        "\n",
        "1. Temporal convolution, which focuses on changes over time for each spatial pixel (e.g. motion).\n",
        "2. Spatial convolution, which looks at image structure in each frame (e.g. shape, position).\n",
        "\n",
        "The figures below highlights the difference between a full 3D convolution kernel versus our\n",
        "spatiotemporal convolution (a.k.a. TENN in the figure below).\n",
        "\n",
        ".. figure:: ../../img/example3Dconv.png\n",
        "   :target: ../../_images/example3Dconv.png\n",
        "   :alt: 3D convolutions\n",
        "   :scale: 80 %\n",
        "   :align: center\n",
        "\n",
        "   3D convolutions example\n",
        "\n",
        ".. figure:: ../../img/exampleT2Sconv.png\n",
        "   :target: ../../_images/exampleT2Sconv.png\n",
        "   :alt: TENN convolutions\n",
        "   :scale: 80 %\n",
        "   :align: center\n",
        "\n",
        "   Spatiotemporal convolutions example\n",
        "\n",
        "This factorized approach reduces compute requirements. In fact, this design proved effective in\n",
        "very different domains: it was applied to gesture videos as well as event-based eye tracking\n",
        "(see tutorial).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. Making it efficient using depthwise separable convolutions\n",
        "\n",
        "To further reduce the computational load of the blocks, we can make them separable, just like\n",
        "depthwise separable convolutions replace full convolutions, reducing computation with minimal\n",
        "accuracy loss, our decomposed temporal-spatial convolutions can also be made separable using\n",
        "an approach inspired by the [MobileNet paper](https://arxiv.org/abs/1704.04861)_. Each layer\n",
        "from the spatiotemporal block is decomposed into 2: the temporal convolution is\n",
        "transformed into a depthwise temporal convolutional layer followed by a pointwise convolutional\n",
        "layer (see figure above), the same is done for the spatial convolution.\n",
        "\n",
        ".. Note::\n",
        "  The design of these spatiotemporal blocks is similar to R(2+1)D blocks, except we place the\n",
        "  temporal layer first. Doing this preserves the temporal richness of the raw input — a critical\n",
        "  decision that avoids \"smearing\" out important movement cues. Moreover, notice that our temporal\n",
        "  layers do not have a stride (compared to R(2+1)D layers).\n",
        "\n",
        ".. figure:: ../../img/comparing_3D_conv_block_designs.png\n",
        "   :target: ../../_images/comparing_3D_conv_block_designs.png\n",
        "   :alt: various types of 3D convolutions\n",
        "   :scale: 60 %\n",
        "   :align: center\n",
        "\n",
        "   Kernel dimensions and strides for various types of 3D convolutions. Dotted lines show depthwise\n",
        "   convolutions. Full lines show full convolutions. Orange outlines are for spatial 3D convs and\n",
        "   purple ones for temporal convolutions.\n",
        "\n",
        "A spatiotemporal block can be easily built using the predefined spatiotemporal\n",
        "blocks from Akida models available through the [akida_models.layer_blocks.spatiotemporal_block](../../api_reference/akida_models_apis.html#akida_models.layer_blocks.spatiotemporal_block)_\n",
        "API.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Building the model: from blocks to network\n",
        "\n",
        "Our gesture recognition model stacks 5 spatiotemporal blocks, forming a shallow yet expressive\n",
        "network. This depth allows the model to:\n",
        "\n",
        "- Gradually capture complex temporal patterns (e.g. \"swipe up\", \"rotate clockwise\")\n",
        "- Downsample spatially to control compute load\n",
        "- Preserve fine-grained timing via non-strided temporal layers\n",
        "- Easily train without skip connections\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_shape = (100, 100, 3)\n",
        "sampling_frequency = 16\n",
        "input_scaling = (127.5, -1.0)\n",
        "n_classes = 27\n",
        "\n",
        "from akida_models.tenn_spatiotemporal import tenn_spatiotemporal_jester\n",
        "model = tenn_spatiotemporal_jester(input_shape=(sampling_frequency,) + input_shape,\n",
        "                                   input_scaling=input_scaling, n_classes=n_classes)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Preserving temporal information\n",
        "As you can see from the summary, the model ends with an 3D average pooling applied only\n",
        "on the spatial dimensions. This ensures that the model can make predictions after the\n",
        "first input frame, preserving fine-grained temporal dynamics and bufferized inference\n",
        "(see section 6.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Gesture classification in videos\n",
        "\n",
        "In this tutorial, we use the [Jester dataset](https://www.qualcomm.com/developer/software/jester-dataset)_,\n",
        "a gesture recognition dataset specifically designed to include movements targeted at human/machine\n",
        "interactions. To do well on the task, information needs to be aggregated across time to accurately\n",
        "separate complex gestures such as clockwise or counterclowise hand turning.\n",
        "\n",
        "The data is available to download in the form of zip files from the\n",
        "[qualcomm website](https://www.qualcomm.com/developer/software/jester-dataset)_ along with\n",
        "[download instructions](https://www.qualcomm.com/content/dam/qcomm-martech/dm-assets/documents/qualcomm-ai-research-jester-download-instructions-v2.pdf)_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Dataset description\n",
        "In the jester dataset, each sample is a short video clip (about 3 seconds) recorded through a\n",
        "webcam with fixed resolution of 100 pixels in height and a frame rate of 12 FPS. There are in\n",
        "total 148,092 videos of 27 different complex gestures covering examples such as \"Zooming Out With\n",
        "2 fingers\", \"Rolling Hand Forward\", \"Shaking Hand\", \"Stop Sign\", \"Swiping Left\", etc..., also\n",
        "including a \"no gesture\" and a \"other movements\" classes.\n",
        "\n",
        "It is a rich and varied dataset with over 1300 different actors performing the gestures.\n",
        "The dataset has determined splits for training, validation and testing with the ratio of\n",
        "80%/10%/10%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Data preprocessing\n",
        "To train the model effectively, we apply minimal preprocessing:\n",
        "\n",
        "- Extract a fixed number of frames (here 16 frames) per sample\n",
        "- Use strided sampling (stride=2) to reduce redundancy and speed up training\n",
        "- Resize the input to a fixed input size (100, 100)\n",
        "- Normalize inputs (between -1 and 1)\n",
        "- Optionally apply an affine transform for training data (ie. randomly and independently apply\n",
        "  translation, scaling, shearing and rotation to each video).\n",
        "\n",
        "The dataset is too large to load completely in a tutorial. If you download the dataset\n",
        "at the links mentioned above, you can load and preprocess it using the get_data API\n",
        "available under akida_models.tenn_spatiotemporal.jester_train.\n",
        "\n",
        "Alternatively, the first few validation samples have been set aside and\n",
        "can be loaded here to demonstration purposes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "\n",
        "# Download and load validation subset from Brainchip data server\n",
        "import os\n",
        "from akida_models import fetch_file\n",
        "from akida_models.tenn_spatiotemporal.jester_train import get_data\n",
        "\n",
        "data_path = fetch_file(\n",
        "    fname=\"jester_subset.tar.gz\",\n",
        "    origin=\"https://data.brainchip.com/dataset-mirror/jester/jester_subset.tar.gz\",\n",
        "    cache_subdir=os.path.join(\"datasets\", \"jester\"), extract=True)\n",
        "data_dir = os.path.join(os.path.dirname(data_path), \"jester_subset\")\n",
        "val_dataset, val_steps = get_data(\"val\", data_dir, sampling_frequency, input_shape[:2], batch_size)\n",
        "\n",
        "# Decode numeric labels into human readable ones: contains all string names for classes\n",
        "# available in the dataset\n",
        "import csv\n",
        "with open(os.path.join(data_dir, \"jester-v1-labels.csv\")) as csvfile:\n",
        "    class_names = [row[0] for row in csv.reader(csvfile)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"classes available are : {class_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training and evaluating the model\n",
        "\n",
        "The model is trained using standard techniques: Adam optimizer, cosine LR scheduler and\n",
        "Categorical Cross-Entropy. We modify the categorical crossentropy slightly to make it \"temporal\":\n",
        "the target class (y-label) is replicated at each time point, thus forcing the model to correctly\n",
        "classify each video frame.\n",
        "\n",
        "Since the training requires a few GPU hours to complete, we will load a pre-trained model for\n",
        "inference. Pre-trained models are available either in floating point or quantized version.\n",
        "First, we'll look at the floating point model, available using the following apis. The evaluation\n",
        "tool is also available to rapidly test the performance on the validation dataset.\n",
        "\n",
        ".. Note: the accuracy here is low because it is computed weighing each time point equally, i.e.\n",
        "         the first frame when the event has not started contributes as much to the predicted label\n",
        "         as a frame with an actual movement in it. The validation accuracy will dramatically\n",
        "         improve once we allow the model to weigh its output in time (see section below).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.model_io import get_model_path, load_model\n",
        "from akida_models.utils import fetch_file\n",
        "from akida_models.tenn_spatiotemporal.jester_train import compile_model\n",
        "\n",
        "model_name_v2 = \"tenn_spatiotemporal_jester.h5\"\n",
        "file_hash_v2 = \"fca52a23152f7c56be1f0db59844a5babb443aaf55babed7669df35b516b8204\"\n",
        "model_path, model_name, file_hash = get_model_path(\"tenn_spatiotemporal\",\n",
        "                                                   model_name_v2=model_name_v2,\n",
        "                                                   file_hash_v2=file_hash_v2)\n",
        "model_path = fetch_file(model_path,\n",
        "                        fname=model_name,\n",
        "                        file_hash=file_hash,\n",
        "                        cache_subdir='models')\n",
        "\n",
        "model = load_model(model_path)\n",
        "compile_model(model, 3e-4, val_steps, 1, sampling_frequency)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "hist = model.evaluate(val_dataset)\n",
        "print(hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Streaming inference: making real-time predictions\n",
        "\n",
        "Once trained, these models can be deployed in online inference mode, making predictions\n",
        "frame-by-frame. This works thanks to:\n",
        "\n",
        "- **causal convolutions**, which ensure that predictions at time *t* use only past and current\n",
        "  frames, not future ones by adding (left-sided) zero-padding. This is critical for streaming\n",
        "  inference where latency matters: we want to be able to make predictions immediately. Our\n",
        "  causal temporal layers don't rely on future frames and start making predictions after the\n",
        "  first frame is received.\n",
        "- **not using a temporal stride**: our model purposefully preserves time information and thus\n",
        "  is able to make a classification guess after each incoming frame.\n",
        "\n",
        "These choices also allow us to configure the spatio-temporal layer in a efficient way using\n",
        "FIFO buffers during inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 FIFO buffering\n",
        "\n",
        "During inference, each temporal layer is replaced with a bufferized 2D convolution: i.e. a\n",
        "Conv2D with an input buffer the size of its kernel (initialized with zeros), handling the\n",
        "streaming input features. Spatial convolutions that have a temporal kernel size of 1 can be\n",
        "seamlessly transformed into 2D convolutions too.\n",
        "\n",
        ".. figure:: ../../img/fifo_buffer.png\n",
        "   :target: ../../_images/fifo_buffer.png\n",
        "   :alt: fifo_buffer\n",
        "   :scale: 80 %\n",
        "   :align: center\n",
        "\n",
        "At its core, a convolution (whether 2D or 3D) involves sliding a small filter (also called\n",
        "a kernel) over the input data and computing a dot product between the filter and a small\n",
        "segment (or window) of the input at each step.\n",
        "\n",
        "To make this process more efficient, we can use a FIFO (First In, First Out) buffer to\n",
        "automatically manage the sliding window. Here's how it works:\n",
        "\n",
        "- The input buffer holds the most recent values from the input signal (top row on the figure\n",
        "  above).\n",
        "- The size of this buffer is equal to the size of the temporal kernel.\n",
        "- After each new incoming values, we perform a dot product between the buffer contents and the\n",
        "  kernel to produce one output value.\n",
        "- Every time a new input value arrives, it's added to the buffer, and the oldest value is\n",
        "  removed.\n",
        "\n",
        "This works seamlessly in causal convolutional networks, where the output at any time step only\n",
        "depends on the current and past input values—not future ones. Because of this causality, the\n",
        "buffer never needs to \"wait\" for future input: it can compute the output as soon as the first\n",
        "frame comes in.\n",
        "\n",
        "**The result?**: Real-time gesture classification, running continuously, with predictions\n",
        "ready after every frame.\n",
        "\n",
        "**How to?** : Akida models provides a simple and easy to use API that transforms compatible\n",
        "spatiotemporal blocks into their equivalent bufferized version found in\n",
        "[akida_models.tenn_spatiotemporal.convert_to_buffer](../../api_reference/akida_models_apis.html#akida_models.tenn_spatiotemporal.convert_to_buffer)_\n",
        "\n",
        ".. Note::\n",
        "\n",
        "  - After conversion, the 3D Convolution layers are transformed into custom\n",
        "    [BufferTempConv](../../api_reference/akida_models_apis.html#akida_models.tenn_spatiotemporal.convert_to_buffer)_\n",
        "    layers.\n",
        "  - As opposed to training where the whole 16 frames samples is passed to the model, the inference\n",
        "    model requires samples to be passed one by one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.tenn_spatiotemporal.convert_spatiotemporal import convert_to_buffer\n",
        "model = convert_to_buffer(model)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The models then can be evaluated on the data using the helper available that passes\n",
        "data frame by frame to the model, accumulating the model's responses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No Akida devices found, running on CPU.\n"
          ]
        }
      ],
      "source": [
        "import akida\n",
        "from cnn2snn import set_akida_version, AkidaVersion\n",
        "# Instantiate akida model\n",
        "with set_akida_version(AkidaVersion.v2):\n",
        "    devices = akida.devices()\n",
        "    if len(devices) > 0:\n",
        "        print(f'Available devices: {[dev.desc for dev in devices]}')\n",
        "        device = devices[0]\n",
        "        print(device.version)\n",
        "        try:\n",
        "            model_akida.map(device)\n",
        "            print(f\"Mapping to Akida device {device.desc}.\")\n",
        "        except Exception as e:\n",
        "            print(\"Model not compatible with FPGA. Running on CPU.\")\n",
        "    else:\n",
        "        print(\"No Akida devices found, running on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from akida_models.tenn_spatiotemporal.jester_train import evaluate_bufferized_model\n",
        "evaluate_bufferized_model(model, val_dataset, val_steps // batch_size, in_akida=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Weighing information\n",
        "\n",
        "The performance of the buffered model is improved because we use a smoothing mecanism on the\n",
        "model's output:\n",
        "\n",
        "- at time *t*, the model's outputs is softmaxed\n",
        "- the softmaxed values from time *t-1* are decayed (using a decay_factor of 0.8)\n",
        "- the two are added\n",
        "\n",
        "This is done across all frames available in the video.\n",
        "The predicted class is only computed once all the frames have been seen by the model for the\n",
        "benchmark, but it is possible for the model to predict the video's class after each new frame.\n",
        "Section 7 below shows an example of this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualizing the predictions of the model in real time\n",
        "\n",
        "Because of this buffering and how the model was trained to output a prediction after each time\n",
        "step, we can effectively visualize the response of the model in time.\n",
        "This part of the tutorial is heavily inspired from the tensorflow tutorial on streaming\n",
        "recognition of gestures based on the [movinet models](https://www.tensorflow.org/hub/tutorials/movinet)_.\n",
        "\n",
        "We pass the data through the trained model frame by frame and collect the predicted classes,\n",
        "applying a softmax on the output of the model.\n",
        "To make the prediction more robust, at each time step we decay the old predictions by a\n",
        "decay_factor so that they contribute less and less to the final predicted class.\n",
        "The decay_factor is an hyperparameter that you can play with. In practice, it slightly improves\n",
        "performance by smoothing the prediction in time and reducing the impact of earlier frames to\n",
        "the final prediction.\n",
        "\n",
        "The video below shows one sample along with the probabilities of the top 5 predictions from\n",
        "our bufferized spatiotemporal model at each time point.\n",
        "\n",
        ".. video:: ../../img/streaming_preds.mp4\n",
        "   :nocontrols:\n",
        "   :autoplay:\n",
        "   :playsinline:\n",
        "   :muted:\n",
        "   :loop:\n",
        "   :width: 50%\n",
        "   :align: center\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Quantizing the model and convertion to akida\n",
        "Once bufferized, the model can be easily quantized with no cost in accuracy. It can then be easily\n",
        "be deployed on hardware for online gesture recognition using the convert method from the cnn2snn\n",
        "package.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Get the calibration data for accurate quantization: these are a subset from the training data.\n",
        "samples = fetch_file(\n",
        "    fname=\"jester_video_bs100.npz\",\n",
        "    origin=\"https://data.brainchip.com/dataset-mirror/samples/jester_video/jester_video_bs100.npz\",\n",
        "    cache_subdir=os.path.join(\"datasets\", \"jester\"), extract=False)\n",
        "samples = os.path.join(os.path.dirname(data_path), \"jester_video_bs100.npz\")\n",
        "data = np.load(samples)\n",
        "samples_arr = np.concatenate([data[item] for item in data.files])\n",
        "num_samples = len(samples_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.layers import QuantizationParams, reset_buffers\n",
        "from quantizeml.models import quantize\n",
        "\n",
        "# Define the quantization parameters and quantize the model\n",
        "qparams = QuantizationParams(activation_bits=8,\n",
        "                             per_tensor_activations=True,\n",
        "                             weight_bits=8,\n",
        "                             input_weight_bits=8,\n",
        "                             input_dtype=\"uint8\")\n",
        "model_quantized = quantize(model, qparams=qparams, samples=samples_arr,\n",
        "                           num_samples=num_samples, batch_size=100, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Evaluate the quantized model\n",
        "evaluate_bufferized_model(model_quantized, val_dataset, val_steps // batch_size, in_akida=False)\n",
        "reset_buffers(model_quantized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Convert to akida\n",
        "from cnn2snn import convert\n",
        "akida_model = convert(model_quantized)\n",
        "akida_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Final thoughts: generalizing the approach\n",
        "\n",
        "Spatiotemporal networks are powerful, lightweight, and flexible. Whether you're building\n",
        "gesture-controlled interfaces or real-time eye-tracking systems, the same design principles\n",
        "apply:\n",
        "\n",
        "- Prioritize temporal modeling early in the network\n",
        "- Use factorized spatiotemporal convolutions for efficiency\n",
        "- Train with augmentation that preserves causality\n",
        "- is seamlessly deployed using streaming inference using FIFO buffers\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
