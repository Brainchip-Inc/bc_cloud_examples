{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# YOLO/PASCAL-VOC detection example\n",
        "\n",
        "This example demonstrates Akida's object detection capabilities using the YOLOv2 architecture.\n",
        "\n",
        "The notebook below is derived from the [\"YOLO/PASCAL-VOC detection tutorial\"](https://doc.brainchipinc.com/examples/general/plot_5_voc_yolo_detection.html) on the Brainchip Developer MetaTF website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Import model\n",
        "\n",
        "The model used for this demonstration can be found at the [Akida 2.0 Model Zoo](https://doc.brainchipinc.com/model_zoo_performance.html#id4). It has been pre-downloaded and converted to Akida here for efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from akida import Model\n",
        "\n",
        "model_akida = Model(\"models/yolo_akidanet_voc_i8_w8_a8.fbz\")\n",
        "model_akida.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Map the model onto the FPGA\n",
        "\n",
        "For more details on this flow, please see: [Model Hardware Mapping](https://doc.brainchipinc.com/user_guide/akida.html#model-hardware-mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import akida\n",
        "from cnn2snn import set_akida_version, AkidaVersion\n",
        "# Instantiate akida model\n",
        "with set_akida_version(AkidaVersion.v2):\n",
        "    devices = akida.devices()\n",
        "    if len(devices) > 0:\n",
        "        print(f'Available devices: {[dev.desc for dev in devices]}')\n",
        "        device = devices[0]\n",
        "        print(device.version)\n",
        "        try:\n",
        "            model_akida.map(device)\n",
        "            print(f\"Mapping to Akida device {device.desc}.\")\n",
        "        except Exception as e:\n",
        "            print(\"Model not compatible with FPGA. Running on CPU.\")\n",
        "    else:\n",
        "        print(\"No Akida devices found, running on CPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3 Load the sample image data\n",
        "\n",
        "Similar to the model, the sample image data and associated anchors have been pre-downloaded for efficiency. You can learn more about the YOLO data that is available by referencing the [Developer MetaTF API Reference](https://doc.brainchipinc.com/api_reference/akida_models_apis.html#yolo). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# Helper function to load and parse the Tfrecord file.\n",
        "def load_tf_dataset(tf_record_file_path):\n",
        "    tfrecord_files = [tf_record_file_path]\n",
        "\n",
        "    # Feature description for parsing the TFRecord\n",
        "    feature_description = {\n",
        "        'image': tf.io.FixedLenFeature([], tf.string),\n",
        "        'objects/bbox': tf.io.VarLenFeature(tf.float32),\n",
        "        'objects/label': tf.io.VarLenFeature(tf.int64),\n",
        "    }\n",
        "\n",
        "    def _count_tfrecord_examples(dataset):\n",
        "        return len(list(dataset.as_numpy_iterator()))\n",
        "\n",
        "    def _parse_tfrecord_fn(example_proto):\n",
        "        example = tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "        # Decode the image from bytes\n",
        "        example['image'] = tf.io.decode_jpeg(example['image'], channels=3)\n",
        "\n",
        "        # Convert the VarLenFeature to a dense tensor\n",
        "        example['objects/label'] = tf.sparse.to_dense(example['objects/label'], default_value=0)\n",
        "\n",
        "        example['objects/bbox'] = tf.sparse.to_dense(example['objects/bbox'])\n",
        "        # Boxes were flattenned that's why we need to reshape them\n",
        "        example['objects/bbox'] = tf.reshape(example['objects/bbox'],\n",
        "                                             (tf.shape(example['objects/label'])[0], 4))\n",
        "        # Create a new dictionary structure\n",
        "        objects = {\n",
        "            'label': example['objects/label'],\n",
        "            'bbox': example['objects/bbox'],\n",
        "        }\n",
        "\n",
        "        # Remove unnecessary keys\n",
        "        example.pop('objects/label')\n",
        "        example.pop('objects/bbox')\n",
        "\n",
        "        # Add 'objects' key to the main dictionary\n",
        "        example['objects'] = objects\n",
        "\n",
        "        return example\n",
        "\n",
        "    # Create a TFRecordDataset\n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
        "    len_dataset = _count_tfrecord_examples(dataset)\n",
        "    parsed_dataset = dataset.map(_parse_tfrecord_fn)\n",
        "\n",
        "    return parsed_dataset, len_dataset\n",
        "\n",
        "\n",
        "labels = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
        "          'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n",
        "          'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',\n",
        "          'train', 'tvmonitor']\n",
        "\n",
        "val_dataset, len_val_dataset = load_tf_dataset(\"datasets/test_20_classes.tfrecord\")\n",
        "print(f\"Loaded VOC2007 sample test data: {len_val_dataset} images.\")\n",
        "\n",
        "with open(\"datasets/coco_anchors.pkl\", 'rb') as handle:\n",
        "        anchors = pickle.load(handle)\n",
        "print(f\"Loaded anchors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4 Run inference on a random image\n",
        "\n",
        "Here, a random image is selected from the sample test data. The image is preprocessed and evaluated through the Akida model. Bounding boxes for detected objects are returned and superimposed on the image using the anchors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from akida_models.detection.processing import preprocess_image, decode_output\n",
        "\n",
        "# Shuffle the data to take a random test image\n",
        "val_dataset = val_dataset.shuffle(buffer_size=len_val_dataset)\n",
        "\n",
        "input_shape = model_akida.layers[0].input_dims\n",
        "\n",
        "# Load the image\n",
        "raw_image = next(iter(val_dataset))['image']\n",
        "\n",
        "# Keep the original image size for later bounding boxes rescaling\n",
        "raw_height, raw_width, _ = raw_image.shape\n",
        "\n",
        "# Pre-process the image\n",
        "image = preprocess_image(raw_image, input_shape)\n",
        "input_image = image[np.newaxis, :].astype(np.uint8)\n",
        "\n",
        "# Call evaluate on the image\n",
        "pots = model_akida.predict(input_image)[0]\n",
        "\n",
        "# Reshape the potentials to prepare for decoding\n",
        "h, w, c = pots.shape\n",
        "pots = pots.reshape((h, w, len(anchors), 4 + 1 + len(labels)))\n",
        "\n",
        "# Decode potentials into bounding boxes\n",
        "raw_boxes = decode_output(pots, anchors, len(labels))\n",
        "\n",
        "# Rescale boxes to the original image size\n",
        "pred_boxes = np.array([[\n",
        "    box.x1 * raw_width, box.y1 * raw_height, box.x2 * raw_width,\n",
        "    box.y2 * raw_height,\n",
        "    box.get_label(),\n",
        "    box.get_score()\n",
        "] for box in raw_boxes])\n",
        "\n",
        "fig = plt.figure(num='VOC detection by Akida')\n",
        "ax = fig.subplots(1)\n",
        "img_plot = ax.imshow(np.zeros(raw_image.shape, dtype=np.uint8))\n",
        "img_plot.set_data(raw_image)\n",
        "\n",
        "for box in pred_boxes:\n",
        "    rect = patches.Rectangle((box[0], box[1]),\n",
        "                             box[2] - box[0],\n",
        "                             box[3] - box[1],\n",
        "                             linewidth=1,\n",
        "                             edgecolor='r',\n",
        "                             facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "    class_score = ax.text(box[0],\n",
        "                          box[1] - 5,\n",
        "                          f\"{labels[int(box[4])]} - {box[5]:.2f}\",\n",
        "                          color='red')\n",
        "\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "akida_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
