{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Efficient online eye tracking with a lightweight spatiotemporal network and event cameras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n",
        "\n",
        "Event cameras are biologically inspired sensors that output asynchronous streams of per-pixel\n",
        "brightness changes, rather than fixed-rate frames. This modality is especially well suited for\n",
        "high-speed, low-power applications like real-time eye tracking on embedded hardware. Traditional\n",
        "deep learning models, however, are often ill-suited for exploiting the unique characteristics of\n",
        "event data — particularly they lack the tools to leverage their temporal precision and sparsity.\n",
        "\n",
        "This tutorial presents a lightweight spatiotemporal neural network architecture designed\n",
        "specifically for online inference on event camera data. The model is:\n",
        "\n",
        "* **Causal and streaming-capable**, using FIFO buffering for minimal-latency inference.\n",
        "* **Highly efficient**, with a small compute and memory footprint.\n",
        "* **Accurate**, achieving state-of-the-art results on a competitive eye tracking benchmark.\n",
        "* **Further optimizable** via activation sparsification, maintaining performance while reducing\n",
        "  computational load.\n",
        "\n",
        "The following sections outline the architecture, dataset characteristics, evaluation results,\n",
        "buffering mechanism, and advanced optimization strategies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Network architecture\n",
        "\n",
        "The proposed architecture is a stack of **spatiotemporal convolutional blocks**, each consisting\n",
        "of a **temporal convolution followed by a spatial convolution**. These are designed to extract\n",
        "both fine-grained temporal features and local spatial structure from event-based input tensors.\n",
        "The figure below shows the details of the model architecture.\n",
        "\n",
        ".. figure:: ../../img/eye_tracking_model_figure.png\n",
        "   :target: ../../_images/eye_tracking_model_figure.png\n",
        "   :alt: Model architecture overview\n",
        "   :scale: 70 %\n",
        "   :align: center\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Key design features\n",
        "\n",
        "1. **Causal Temporal Convolutions**\n",
        "\n",
        "   Temporal convolutions are strictly causal—output at time *t* depends only on input at time ≤\n",
        "   *t*. This property is critical for real-time, online inference, allowing inference from the\n",
        "   first received frame from the sensor.\n",
        "\n",
        "2. **Factorized 3D Convolution Scheme**\n",
        "\n",
        "   Our spatiotemporal blocks perform temporal convolutions first, followed by spatial\n",
        "   convolutions. Decomposing the 3D convolutions into temporal and spatial layers greatly\n",
        "   reduces computation (in much the same way that depthwise separable layers do for 2D\n",
        "   convolutions).\n",
        "\n",
        "   .. figure:: ../../img/eye_tracking_block_description.png\n",
        "      :target: ../../_images/eye_tracking_block_description.png\n",
        "      :alt: Factorization of conv3D into temporal and spatial convolutions\n",
        "      :scale: 80 %\n",
        "      :align: center\n",
        "\n",
        "3. **Depthwise-Separable Convolutions (DWS)**\n",
        "\n",
        "   Both temporal and spatial layers can optionally be configured as depthwise-separable to\n",
        "   further reduce computation with minimal loss in accuracy.\n",
        "\n",
        "4. **No Residual Connections**\n",
        "\n",
        "   To conserve memory and simplify deployment on edge devices, residual connections are omitted.\n",
        "   Since the model has a reduced number of layers, they are not critical to achieve SOTA\n",
        "   performance.\n",
        "\n",
        "5. **Detection Head**\n",
        "\n",
        "   A lightweight head, inspired by CenterNet [Zhou et al. 2019](https://arxiv.org/abs/1904.07850)_, predicts a confidence score and local spatial offsets\n",
        "   for the pupil position over a coarse spatial grid. The predicted position of the pupil can\n",
        "   then be reconstructed.\n",
        "\n",
        "   .. figure:: ../../img/eye_tracking_post_processing.png\n",
        "      :target: ../../_images/eye_tracking_post_processing.png\n",
        "      :alt: Centernet head and post processing\n",
        "      :scale: 80 %\n",
        "      :align: center\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Instantiating the spatiotemporal blocks\n",
        "\n",
        "QuantizeML and Akida Models natively work with Tensorflow/Keras layers: akida_models has all the\n",
        "necessary functions to instantiate a network based on spatiotemporal layers as well as training\n",
        "pipelines available to train models on the jester dataset, the dvs128 dataset or this dataset.\n",
        "\n",
        "In this tutorial, we'll use PyTorch and introduce the\n",
        "[tenns_modules](https://pypi.org/project/tenns-modules/)_ package which is available to create\n",
        "Akida compatible spatiotemporal blocks. The package contains a [spatio-temporal block](../../api_reference/tenns_modules_apis.html#tenns_modules.SpatioTemporalBlock)_\n",
        "composed of a [spatial](../../api_reference/tenns_modules_apis.html#tenns_modules.SpatialBlock)_\n",
        "and a [temporal](../../api_reference/tenns_modules_apis.html#tenns_modules.TemporalBlock)_\n",
        "block.\n",
        "\n",
        "The code below shows how to instantiate the simple 10 layers architecture we used to track the\n",
        "pupil coordinates in time using the tenns_modules package.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Show how to load and create the model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from tenns_modules import SpatioTemporalBlock\n",
        "from torchinfo import summary\n",
        "\n",
        "n_depthwise_layers = 4\n",
        "channels = [2, 8, 16, 32, 48, 64, 80, 96, 112, 128, 256]\n",
        "t_kernel_size = 5  # can vary from 1 to 10\n",
        "s_kernel_size = 3  # can vary in [1, 3, 5, 7] (1 only when depthwise is False)\n",
        "\n",
        "\n",
        "class TennSt(nn.Module):\n",
        "    def __init__(self, channels, t_kernel_size, s_kernel_size, n_depthwise_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        depthwises = [False] * (10 - n_depthwise_layers) + [True] * n_depthwise_layers\n",
        "        self.backbone = nn.Sequential()\n",
        "        for i in range(0, len(depthwises), 2):\n",
        "            in_channels, med_channels, out_channels = channels[i], channels[i + 1], channels[i + 2]\n",
        "            t_depthwise, s_depthwise = depthwises[i], depthwises[i]\n",
        "\n",
        "            self.backbone.append(\n",
        "                SpatioTemporalBlock(in_channels=in_channels, med_channels=med_channels,\n",
        "                                    out_channels=out_channels, t_kernel_size=t_kernel_size,\n",
        "                                    s_kernel_size=s_kernel_size, s_stride=2, bias=False,\n",
        "                                    t_depthwise=t_depthwise, s_depthwise=s_depthwise))\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            SpatioTemporalBlock(channels[-1], channels[-1], channels[-1],\n",
        "                                t_kernel_size=t_kernel_size, s_kernel_size=s_kernel_size,\n",
        "                                t_depthwise=False, s_depthwise=False),\n",
        "            nn.Conv3d(channels[-1], 3, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.head((self.backbone(input)))\n",
        "\n",
        "\n",
        "model = TennSt(channels, t_kernel_size, s_kernel_size, n_depthwise_layers)\n",
        "summary(model, input_size=(1, 2, 50, 96, 128), depth=4, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset and preprocessing\n",
        "\n",
        "The model is trained and evaluated on the\n",
        "[AIS 2024 Event-Based Eye Tracking Challenge Dataset](https://www.kaggle.com/competitions/event-based-eye-tracking-ais2024)_, which contains\n",
        "recordings from 13 participants, captured using 480×640-resolution event camera. Each participant\n",
        "has between 2 and 6 recording sessions. The ground truth pupil (x- and y-) coordinates are\n",
        "provided at a resolution of 100Hz. The evaluation of the predictions is done at 20Hz at a\n",
        "resolution of 60x80 when the eyes are opened.\n",
        "\n",
        "The video below shows you an example of the reconstructed frames (note that the video has been\n",
        "sped up). The ground truth pupil location is represented by a cross: the cross is green when the\n",
        "eye is opened and it turns red when the eye closes.\n",
        "\n",
        ".. video:: ../../img/eye_tracking_valdata_gt_only_fast.mp4\n",
        "   :nocontrols:\n",
        "   :autoplay:\n",
        "   :playsinline:\n",
        "   :muted:\n",
        "   :loop:\n",
        "   :width: 50%\n",
        "   :align: center\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Preprocessing\n",
        "\n",
        "The following preprocessing is applied to the event data:\n",
        "\n",
        "- temporal augmentations (for training only)\n",
        "- spatial downsampling (by 5) and event binning to create segments with fixed temporal length\n",
        "- spatial affine transforms\n",
        "- frames where the eye is labeled as closed are ignored during training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.1 Event binning\n",
        "\n",
        "Events are represented as 4-tuples: *(polarity, x, y, timestamp)*. These are converted into\n",
        "tensors of shape **(P=2, T, H, W)** using **causal event volume binning**, a method that preserves\n",
        "temporal fidelity while avoiding future context. Binning uses a causal triangle kernel to\n",
        "approximate each event’s influence over space and time, as you can see from the graph below.\n",
        "\n",
        ".. figure:: ../../img/eye_tracking_causal_event_binning.png\n",
        "   :target: ../../_images/eye_tracking_causal_event_binning.png\n",
        "   :alt: Example of causal event binning\n",
        "   :scale: 80 %\n",
        "   :align: center\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.1.2 Augmentation\n",
        "\n",
        "To improve generalization in a data-limited regime, the following transforms are applied to the\n",
        "events (and the corresponding pupil coordinates) during training only:\n",
        "\n",
        "* **Spatial affine transforms** are applied such as scaling, rotation, translation.\n",
        "* **Temporal augmentations** including random time scaling and flipping (with polarity inversion).\n",
        "* **Random temporal flip** with probability 0.5 is applied to the time and polarity dimension.\n",
        "\n",
        "These transforms are applied to each segment independently (but not varied within a segment).\n",
        "For better legibility, the dataset was preprocessed offline and made available for evaluation\n",
        "purposes only.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Evaluation metric\n",
        "\n",
        "For the competition, the primary metric for model evaluation was the “p10” accuracy: the\n",
        "percentage of predictions falling within 10 pixels of the ground truth (i.e. if the predicted\n",
        "pupil center falls within the blue dashed circle in the figure below). We can also consider more\n",
        "stringent measures, such as a p3 accuracy (3 pixels); or simpler linear measures, such as the\n",
        "Euclidean distance (L2).\n",
        "\n",
        ".. figure:: ../../img/eye_tracking_pixel_accuracy_euclidean.png\n",
        "   :target: ../../_images/eye_tracking_pixel_accuracy_euclidean.png\n",
        "   :alt: Metrics used in the competition\n",
        "   :scale: 80 %\n",
        "   :align: center\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model training & evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Training details\n",
        "\n",
        "The following hyperparameters were used for training:\n",
        "\n",
        "- batch size of 32\n",
        "- 50 event frames per segment\n",
        "- 200 epochs\n",
        "- AdamW optimizer with base LR of 0.002 and weight decay of 0.005\n",
        "- learning rate scheduler with linear warm up (for 2.5% of total epochs) and a cosine decay\n",
        "\n",
        ".. Note::\n",
        "  We don't train the model here as it requires access to a GPU but rather load a pre-trained model\n",
        "  for convenience.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the pretrained weights in our model\n",
        "from akida_models import fetch_file\n",
        "\n",
        "ckpt_file = fetch_file(\n",
        "    fname=\"tenn_spatiotemporal_eye.ckpt\",\n",
        "    origin=\"https://data.brainchip.com/models/AkidaV2/tenn_spatiotemporal/tenn_spatiotemporal_eye.ckpt\",\n",
        "    cache_subdir='models')\n",
        "\n",
        "checkpoint = torch.load(ckpt_file, map_location=\"cpu\")\n",
        "new_state_dict = {k.replace('model._orig_mod.', ''): v for k, v in checkpoint[\"state_dict\"].items()}\n",
        "model.load_state_dict(new_state_dict)\n",
        "_ = model.eval().cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import akida\n",
        "from cnn2snn import set_akida_version, AkidaVersion\n",
        "# Instantiate akida model\n",
        "with set_akida_version(AkidaVersion.v2):\n",
        "    devices = akida.devices()\n",
        "    if len(devices) > 0:\n",
        "        print(f'Available devices: {[dev.desc for dev in devices]}')\n",
        "        device = devices[0]\n",
        "        print(device.version)\n",
        "        try:\n",
        "            model.map(device)\n",
        "            print(f\"Mapping to Akida device {device.desc}.\")\n",
        "        except Exception as e:\n",
        "            print(\"Model not compatible with FPGA. Running on CPU.\")\n",
        "    else:\n",
        "        print(\"No Akida devices found, running on CPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Evaluation\n",
        "\n",
        "The preprocessed test data have been set aside and can be loaded from the archive available\n",
        "online.\n",
        "\n",
        ".. Note::\n",
        "  To optimize storage and reduce processing time, only the first 400 frames from each test file\n",
        "  have been mirrored on the dataset server. This subset is representative and sufficient for\n",
        "  evaluation purposes in this tutorial.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "samples = fetch_file(\"https://data.brainchip.com/dataset-mirror/eye_tracking_ais2024_cvpr/eye_tracking_preprocessed_400frames_test.npz\",\n",
        "                     fname=\"eye_tracking_preprocessed_400frames_test.npz\")\n",
        "data = np.load(samples, allow_pickle=True)\n",
        "events, centers = data[\"events\"], data[\"centers\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate the model, we pass the data through our spatiotemporal model. Once we have the output,\n",
        "we need to post process the model's output to reconstruct the predicted pupil coordinates in the\n",
        "prediction space (60, 80).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def process_detector_prediction(pred):\n",
        "    \"\"\"Post-processing of model predictions to extract the predicted pupil coordinates for a model\n",
        "    that has a centernet like head.\n",
        "\n",
        "    Args:\n",
        "        preds (torch.Tensor): shape (B, C, T, H, W)\n",
        "\n",
        "    Returns:\n",
        "        torch tensor of (B, 2) containing the x and y predicted coordinates\n",
        "    \"\"\"\n",
        "    torch_device = pred.device\n",
        "    batch_size, _, frames, height, width = pred.shape\n",
        "    # Extract the center heatmap, and the x and y offset maps\n",
        "    pred_pupil, pred_x_mod, pred_y_mod = pred.moveaxis(1, 0)\n",
        "    pred_x_mod = torch.sigmoid(pred_x_mod)\n",
        "    pred_y_mod = torch.sigmoid(pred_y_mod)\n",
        "\n",
        "    # Find the stronger peak in the center heatmap and it's coordinates\n",
        "    pupil_ind = pred_pupil.flatten(-2, -1).argmax(-1)  # (batch, frames)\n",
        "    pupil_ind_x = pupil_ind % width\n",
        "    pupil_ind_y = pupil_ind // width\n",
        "\n",
        "    # Reconstruct the predicted offset\n",
        "    batch_range = torch.arange(batch_size, device=torch_device).repeat_interleave(frames)\n",
        "    frames_range = torch.arange(frames, device=torch_device).repeat(batch_size)\n",
        "    pred_x_mod = pred_x_mod[batch_range, frames_range, pupil_ind_y.flatten(), pupil_ind_x.flatten()]\n",
        "    pred_y_mod = pred_y_mod[batch_range, frames_range, pupil_ind_y.flatten(), pupil_ind_x.flatten()]\n",
        "\n",
        "    # Express the coordinates in size agnostic terms (between 0 and 1)\n",
        "    x = (pupil_ind_x + pred_x_mod.view(batch_size, frames)) / width\n",
        "    y = (pupil_ind_y + pred_y_mod.view(batch_size, frames)) / height\n",
        "    return torch.stack([x, y], dim=1)\n",
        "\n",
        "\n",
        "def compute_distance(pred, center):\n",
        "    \"\"\"Computes the L2 distance for a prediction and center matrice\n",
        "\n",
        "    Args:\n",
        "        pred: torch tensor of shape (2, T)\n",
        "        center: torch tensor of shape (2, T)\n",
        "    \"\"\"\n",
        "    height, width = 60, 80\n",
        "    pred = pred.detach().clone()\n",
        "    center = center.detach().clone()\n",
        "    pred[0, :] *= width\n",
        "    pred[1, :] *= height\n",
        "    center[0, :] *= width\n",
        "    center[1, :] *= height\n",
        "    l2_distances = torch.norm(center - pred, dim=0)\n",
        "    return l2_distances\n",
        "\n",
        "\n",
        "def pretty_print_results(collected_distances):\n",
        "    \"\"\"Prints the distance and accuracy within different pixel tolerance.\n",
        "\n",
        "    By default, only the results at 20Hz will be printed (to be compatible with the\n",
        "    metrics of the challenge). To print the results computed on the whole trial,\n",
        "    use downsample=False. In practice, this changes very little to the final performance\n",
        "    of the model.\n",
        "    \"\"\"\n",
        "    for t in [10, 5, 3, 1]:\n",
        "        p_acc = (collected_distances < t).sum() / collected_distances.size\n",
        "        print(f'- p{t}: {p_acc:.3f}')\n",
        "    print(f'- Euc. Dist: {collected_distances.mean():.3f} ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get the model device to propagate the events properly\n",
        "torch_device = next(model.parameters()).device\n",
        "\n",
        "# Compute the distances across all 9 trials\n",
        "collected_l2_distances = np.zeros((0,))\n",
        "for trial_idx, event in enumerate(events):\n",
        "    center = torch.from_numpy(centers[trial_idx]).float().to(torch_device)\n",
        "    event = torch.from_numpy(event).unsqueeze(0).float().to(torch_device)\n",
        "    pred = model(event)\n",
        "    pred = process_detector_prediction(pred).squeeze(0)\n",
        "    l2_distances = compute_distance(pred, center)\n",
        "    collected_l2_distances = np.concatenate((collected_l2_distances, l2_distances), axis=0)\n",
        "\n",
        "pretty_print_results(collected_l2_distances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Official competition results\n",
        "\n",
        "The results for the competition are on the test set (labels are not available). The main metric in\n",
        "the challenge was the p10. Using this metric, our model ranked 3rd (see table below copied from\n",
        "the [original challenge survey paper](https://openaccess.thecvf.com/content/CVPR2024W/AI4Streaming/papers/Wang_Event-Based_Eye_Tracking._AIS_2024_Challenge_Survey_CVPRW_2024_paper.pdf)_).\n",
        "\n",
        "However, other metrics were reported in the original challenge survey: the accuracy within 5 (p5),\n",
        "3 (p3) or 1 pixel (p1), as well as metrics directly measuring the distance between ground truth\n",
        "and predicted pupil location (L2 and L1, i.e. smaller values are better). On these more stringent\n",
        "metrics, our model outperforms the other models on all the other metrics.\n",
        "\n",
        ".. list-table::\n",
        "   :header-rows: 1\n",
        "\n",
        "   * - **Team**\n",
        "     - **Rank**\n",
        "     - p10 private (primary)\n",
        "     - p10 🡑\n",
        "     - p5 🡑\n",
        "     - p3 🡑\n",
        "     - p1 🡑\n",
        "     - *L2* 🡓\n",
        "     - *L1* 🡓\n",
        "   * - USTCEventGroup\n",
        "     - 1\n",
        "     - **99.58**\n",
        "     - **99.42**\n",
        "     - 97.05\n",
        "     - 90.73\n",
        "     - 33.75\n",
        "     - 1.67\n",
        "     - 2.11\n",
        "   * - FreeEvs\n",
        "     - 2\n",
        "     - 99.27\n",
        "     - 99.26\n",
        "     - 94.31\n",
        "     - 83.83\n",
        "     - 23.91\n",
        "     - 2.03\n",
        "     - 2.56\n",
        "   * - **Brainchip**\n",
        "     - 3\n",
        "     - 99.16\n",
        "     - 99.00\n",
        "     - **97.79**\n",
        "     - **94.58**\n",
        "     - **45.50**\n",
        "     - **1.44**\n",
        "     - **1.82**\n",
        "   * - Go Sparse\n",
        "     - 4\n",
        "     - 98.74\n",
        "     - 99.00\n",
        "     - 77.20\n",
        "     - 47.97\n",
        "     - 7.32\n",
        "     - 3.51\n",
        "     - 4.63\n",
        "   * - MeMo\n",
        "     - 4\n",
        "     - 98.74\n",
        "     - 99.05\n",
        "     - 89.36\n",
        "     - 50.87\n",
        "     - 6.53\n",
        "     - 3.2\n",
        "     - 4.04\n",
        "\n",
        "The best metric in class is highlighted in bold, 🡑 means higher values are best, 🡓 means lower\n",
        "values are best.\n",
        "\n",
        "The code below shows an inference on the model using the *test* dataset. Note that the\n",
        "results below differ from the challenge metrics reported above because our submission model was\n",
        "trained on both the train and validation data to achieve the best possible performance (as allowed\n",
        "by the rules), but the model below that was used for the ablation studies was trained on the train\n",
        "set only.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Ablation studies and efficiency optimization\n",
        "\n",
        "Figure reproduced from the original paper.\n",
        "\n",
        ".. figure:: ../../img/paper_figure3.png\n",
        "   :target: ../../_images/paper_figure3.png\n",
        "   :alt: Figure 3 from the original paper\n",
        "   :scale: 80 %\n",
        "   :align: center\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Ablation studies\n",
        "\n",
        "To test the robustness of our design choices, we performed a series of ablation studies. To\n",
        "provide a baseline model for the ablation study, we trained a model on the 'train' split only and\n",
        "tested it on the validation dataset. This model gets a p10 of 0.963 and an l2 distance of 2.79.\n",
        "\n",
        "This showed that:\n",
        "\n",
        "1. Removing spatial affine augmentation reduces performance dramatically (from 0.963 → 0.588).\n",
        "2. Causal event binning performs equivalently to other methods while enabling streaming inference.\n",
        "3. Larger temporal kernels (e.g., size 5 vs. 3) offer small but consistent improvements in\n",
        "   accuracy.\n",
        "4. Using only batch normalization (BN) layers gave a small improvement over group norm (GN) only\n",
        "   or a mix of BN/GN(96.9 vs 96.0 or 96.3).\n",
        "\n",
        "For more details you can refer to the [paper](https://openaccess.thecvf.com/content/CVPR2024W/AI4Streaming/papers/Pei_A_Lightweight_Spatiotemporal_Network_for_Online_Eye_Tracking_with_Event_CVPRW_2024_paper.pdf)_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Efficiency-accuracy trade-offs\n",
        "\n",
        "In certain environments, such as edge or low-power devices, the balance between model size and\n",
        "computational demand often matters more than achieving state-of-the-art accuracy. This section\n",
        "explores the trade-off between maximizing accuracy and maintaining model efficiency along 3 axis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2.1 Spatial resolution\n",
        "\n",
        "We looked at how reducing input image size affects model performance (see [figure 3.A](./plot_1_eye_tracking_cvpr.html#ablation-studies-and-efficiency-optimization)_). Even with an\n",
        "input size of 60 x 80 (downsampling by a factor of 8), the model still performs almost as well\n",
        "as with our default setting (downsampling by a factor of 5), while requiring only a third of the\n",
        "computation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2.2 Depthwise separable convolutions\n",
        "\n",
        "From the outset, we decided to further decompose our factorized convolutions into depthwise and\n",
        "pointwise convolutions (similar to depthwise separable convolutions introduced in MobileNet V1).\n",
        "We explored how these impacted model performance (see [figure 3.B](./plot_1_eye_tracking_cvpr.html#ablation-studies-and-efficiency-optimization)_): as the number\n",
        "of separable convolutions used increases, the MACs of the model decrease, with a relatively small\n",
        "impact on the validation distance. When no separable layers are used, the final validation\n",
        "distance is 2.6 vs. 3.1 when all layers are separable. Our baseline model had the last 4 layers\n",
        "configured as separable. Changing just 2 more to separable could lead to a reduction of almost 30%\n",
        "in compute, with almost no impact on performance (compare the turquoise with green lines on the\n",
        "[figure 3.B](./plot_1_eye_tracking_cvpr.html#ablation-studies-and-efficiency-optimization)_).\n",
        "\n",
        "The combination of these techniques results in a highly efficient model with a computational cost\n",
        "of just 55M MACs/frame, and even less when sparsity is exploited.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2.3 Activity regularization\n",
        "\n",
        "Event camera data is inherently sparse. However, intermediate layers in a neural network may still\n",
        "produce dense activations unless explicitly regularized. When measuring the baseline sparsity in\n",
        "the network, we found it to be on average 50% (about what one would expect given ReLU activation\n",
        "functions), much of which may not be informative given the very high spatial sparsity of the input\n",
        "to the network. By applying L1 regularization to ReLU activations during training, the model is\n",
        "encouraged to silence unnecessary activations. We applied 5 different levels of regularization to\n",
        "our model: [figure 3.C](./plot_1_eye_tracking_cvpr.html#ablation-studies-and-efficiency-optimization)_ shows how the\n",
        "average distance varies depending on the regularization strength while [figure 3.D](./plot_1_eye_tracking_cvpr.html#ablation-studies-and-efficiency-optimization)_ shows how the\n",
        "sparse aware MACs (i.e. MACs multiplied by the model's mean sparsity per layer) is affected by\n",
        "regularization. We can see that over 90% activation sparsity is achievable with a negligible\n",
        "performance degradation (p10 remains >0.96).\n",
        "\n",
        "This is especially interesting because Akida is an event based hardware: it is capable of skipping\n",
        "zero operations. In such hardware, high level of activation sparsity can translate into ~5× speedups.\n",
        "\n",
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>Based on these ablation studies, the model made available through the\n",
        "  [model zoo](../../model_zoo_performance.html#eye-tracking)_ has been optimized for the inference\n",
        "  on Akida Hardware (downsampling by a factor of 6, use of depthwise separable convolutions), so the\n",
        "  number of parameters and accuracy reported differ.</p></div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. FIFO buffering for streaming inference\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Key mechanism\n",
        "\n",
        "Each temporal convolutional layer maintains a fixed-length FIFO buffer of its input history (equal\n",
        "to the kernel size). At each time step:\n",
        "\n",
        "- The buffer is updated with the newest frame.\n",
        "- A dot product is computed between the buffer contents and the kernel weights.\n",
        "- The result is passed through normalization and spatial convolution.\n",
        "\n",
        "This approach mimics the operation of a sliding temporal convolution but avoids recomputation and memory\n",
        "redundancy, ensuring minimal latency and efficient real-time processing.\n",
        "For more details of this approach, see the tutorial that [introduced spatiotemporal models](./plot_0_introduction_to_spatiotemporal_models.html#streaming-inference-making-real-time-predictions)_.\n",
        "\n",
        ".. figure:: ../../img/fifo_buffer.png\n",
        "   :target: ../../_images/fifo_buffer.png\n",
        "   :alt: Fifo buffer\n",
        "   :scale: 80 %\n",
        "   :align: center\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Exporting to ONNX\n",
        "\n",
        "The transformation to buffer mode is done during quantization step (see dedicated section below).\n",
        "The first step is to export the model to ONNX format. This is made very easy using the\n",
        "[tenns_modules](https://pypi.org/project/tenns-modules/)_ package and the [export_to_onnx](../../api_reference/tenns_modules_apis.html#tenns_modules.export_to_onnx)_ function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tenns_modules import export_to_onnx\n",
        "\n",
        "# Using a batch size of 10 to export with a dynamic batch size\n",
        "onnx_checkpoint_path = \"tenns_modules_onnx.onnx\"\n",
        "export_to_onnx(model, (10, 2, 50, 96, 128), out_path=onnx_checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the ONNX model that was automatically saved\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "\n",
        "model = onnx.load(onnx_checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Quantization and conversion to Akida\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Quantization\n",
        "\n",
        "To be deployable on Akida, the model needs to be quantized. This can easily be done using the\n",
        "QuantizeML package. For more details on the quantization scheme with the ONNX package see this\n",
        "example on [off-the-shelf model quantization](../quantization/plot_2_off_the_shelf_quantization.html)_.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import quantize\n",
        "from quantizeml.layers import QuantizationParams\n",
        "\n",
        "# Retrieve calibration samples:\n",
        "samples = fetch_file(\"https://data.brainchip.com/dataset-mirror/samples/eye_tracking/eye_tracking_onnx_samples_bs100.npz\",\n",
        "                     fname=\"eye_tracking_onnx_samples_bs100.npz\")\n",
        "\n",
        "# Define quantization parameters and load quantization samples\n",
        "qparams = QuantizationParams(per_tensor_activations=True, input_dtype='int8')\n",
        "data = np.load(samples)\n",
        "samples = np.concatenate([data[item] for item in data.files])\n",
        "\n",
        "# Quantize the model\n",
        "model_quant = quantize(model, qparams=qparams, epochs=1, batch_size=100, samples=samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n",
        "  During this step, the model is also bufferized, meaning that the FIFOs of the temporal\n",
        "  convolutions are automatically created and initialized from the 3D convolutions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 ONNX model evaluation\n",
        "\n",
        "This model can be evaluated using the same process as before with a few differences:\n",
        "\n",
        "- We need to pass each frame to the model independently (i.e. the model now has a 4-D input shape\n",
        "  (B, C, H,  W) - batch, channels, height, width).\n",
        "- The post processing function needs to be modified to use numpy functions (instead of torch)\n",
        "- Once all frames from a given trial have been passed through, the FIFO buffers of the temporal\n",
        "  convolutions need to be reset using the [reset_buffers](../../api_reference/quantizeml_apis.html#quantizeml.models.reset_buffers)_ available from\n",
        "  QuantizeML.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def custom_process_detector_prediction(pred):\n",
        "    \"\"\" Post-processing of the model's output heatmap.\n",
        "\n",
        "    Reconstructs the predicted x- and y- center location using numpy functions to post-process\n",
        "    the output of a ONNX model.\n",
        "    \"\"\"\n",
        "    def sigmoid(x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "    # Pred shape is (batch, channels, height, width)\n",
        "    batch_size, _, height, width = pred.shape\n",
        "\n",
        "    # Split channels - reshape to move frames dimension after batch\n",
        "    # Now (batch, height, width, channels)\n",
        "    pred = np.moveaxis(pred, 1, -1)\n",
        "    pred_pupil = pred[..., 0]\n",
        "    pred_x_mod = sigmoid(pred[..., 1])\n",
        "    pred_y_mod = sigmoid(pred[..., 2])\n",
        "\n",
        "    # Find pupil location\n",
        "    pred_pupil_flat = pred_pupil.reshape(batch_size, -1)\n",
        "    pupil_ind = np.argmax(pred_pupil_flat, axis=-1)\n",
        "    pupil_ind_x = pupil_ind % width\n",
        "    pupil_ind_y = pupil_ind // width\n",
        "\n",
        "    # Get the learned x- y- offset\n",
        "    batch_idx = np.repeat(np.arange(batch_size)[:, None], 1, axis=1)\n",
        "    x_mods = pred_x_mod[batch_idx, pupil_ind_y, pupil_ind_x]\n",
        "    y_mods = pred_y_mod[batch_idx, pupil_ind_y, pupil_ind_x]\n",
        "\n",
        "    # Calculate final coordinates\n",
        "    x = (pupil_ind_x + x_mods) / width\n",
        "    y = (pupil_ind_y + y_mods) / height\n",
        "\n",
        "    return np.stack([x, y], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the inference session for the ONNX model and evaluate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from onnxruntime import InferenceSession, SessionOptions\n",
        "from onnxruntime_extensions import get_library_path\n",
        "from quantizeml.onnx_support.quantization import ONNXModel\n",
        "\n",
        "sess_options = SessionOptions()\n",
        "sess_options.register_custom_ops_library(get_library_path())\n",
        "model_quant = ONNXModel(model_quant)\n",
        "session = InferenceSession(model_quant.serialized, sess_options=sess_options,\n",
        "                           providers=['CPUExecutionProvider'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from quantizeml.models import reset_buffers\n",
        "from tqdm import tqdm\n",
        "\n",
        "# And then evaluate the model\n",
        "collected_l2_distances = []\n",
        "for trial_idx, event in enumerate(events):\n",
        "    center = centers[trial_idx]\n",
        "    for frame_idx in tqdm(range(event.shape[1])):\n",
        "        frame = event[:, frame_idx, ...][None, ...].astype(np.float32)\n",
        "        pred = session.run(None, {model_quant.input[0].name: frame})[0]\n",
        "        pred = custom_process_detector_prediction(pred).squeeze()\n",
        "        y_pred_x = pred[0] * 80\n",
        "        y_pred_y = pred[1] * 60\n",
        "        center_x = center[0, frame_idx] * 80\n",
        "        center_y = center[1, frame_idx] * 60\n",
        "        collected_l2_distances.append(np.sqrt(np.square(\n",
        "            center_x - y_pred_x) + np.square(center_y - y_pred_y)))\n",
        "    # Reset FIFOs between each file\n",
        "    reset_buffers(model_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pretty_print_results(np.array(collected_l2_distances))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Conversion to Akida\n",
        "\n",
        "The quantized model can be easily converted to Akida using the cnn2snn package.\n",
        "The [convert](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_ function\n",
        "returns a model in Akida format ready for inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from cnn2snn import convert\n",
        "\n",
        "akida_model = convert(model_quant.model)\n",
        "akida_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n",
        "  - For more information you can refer to the paper available [here](https://openaccess.thecvf.com/content/CVPR2024W/AI4Streaming/papers/Pei_A_Lightweight_Spatiotemporal_Network_for_Online_Eye_Tracking_with_Event_CVPRW_2024_paper.pdf)_.\n",
        "  - There is also a full training pipeline available in tensorflow/Keras from the akida_models\n",
        "    package that reproduces the performance presented in the paper available with the\n",
        "    [akida_models.tenn_spatiotemporal](../../api_reference/akida_models_apis.html#akida_models.tenn_spatiotemporal_eye)_ function.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "akida_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
